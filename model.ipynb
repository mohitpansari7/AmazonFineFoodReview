{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('./database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_sql_query('''SELECT * FROM Reviews WHERE Score != 3 LIMIT 5000''', con)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(score):\n",
    "    if score > 3:\n",
    "        return 'positive'\n",
    "    return 'negative'\n",
    "\n",
    "actualScore = data['Score']\n",
    "positiveNegative = actualScore.map(partition)\n",
    "data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
      "0                     1                       1  positive  1303862400   \n",
      "1                     0                       0  negative  1346976000   \n",
      "2                     1                       1  positive  1219017600   \n",
      "3                     3                       3  negative  1307923200   \n",
      "4                     0                       0  positive  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "\n",
    "-Deduplication\n",
    "-Removing false records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4986, 10)\n"
     ]
    }
   ],
   "source": [
    "sortedData = data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "deDuplicatedData = sortedData.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'}, keep='first', inplace=False)\n",
    "print(deDuplicatedData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now removing those records where HelpfulnessNumerator > HelpfulnessDenominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalData = deDuplicatedData[deDuplicatedData.HelpfulnessDenominator >= deDuplicatedData.HelpfulnessNumerator]\n",
    "finalData.shape\n",
    "#print(finalData['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chips Ahoy, chewy oatmeal cookies are so moist that it is difficult to separate a single cookie from the package without it breaking into pieces.  My tastebuds say they are too sweet, not cooked enough, lack ample chocolate chips and oatmeal flakes.  If you like cookie dough, you most likely will enjoy these.  I'll stick with a crunchy, less sweet cookie.\n"
     ]
    }
   ],
   "source": [
    "data1501 = finalData['Text'].values[1501]\n",
    "print(data1501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML tags from data using bs4's Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chips Ahoy, chewy oatmeal cookies are so moist that it is difficult to separate a single cookie from the package without it breaking into pieces.  My tastebuds say they are too sweet, not cooked enough, lack ample chocolate chips and oatmeal flakes.  If you like cookie dough, you most likely will enjoy these.  I'll stick with a crunchy, less sweet cookie.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(data1501, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decontracting i.e. changing don't to do not ,.... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracting(str):\n",
    "    str = re.sub(r\"wont't\", 'will not', str)\n",
    "    str = re.sub(r\"can\\'t\", 'cannot', str)\n",
    "\n",
    "    str = re.sub(r\"n\\'t\", \" not\", str)\n",
    "    str = re.sub(r\"\\'re\", \" are\", str)\n",
    "    str = re.sub(r\"\\'s\", \" is\", str)\n",
    "    str = re.sub(r\"\\'d\", \" would\", str)\n",
    "    str = re.sub(r\"\\'ll\", \" will\", str)\n",
    "    str = re.sub(r\"\\'t\", \" not\", str)\n",
    "    str = re.sub(r\"\\'ve\", \" have\", str)\n",
    "    str = re.sub(r\"\\'m\", \" am\", str)\n",
    "    return str    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chips Ahoy, chewy oatmeal cookies are so moist that it is difficult to separate a single cookie from the package without it breaking into pieces.  My tastebuds say they are too sweet, not cooked enough, lack ample chocolate chips and oatmeal flakes.  If you like cookie dough, you most likely will enjoy these.  I will stick with a crunchy, less sweet cookie.\n"
     ]
    }
   ],
   "source": [
    "data1501 = decontracting(data1501)\n",
    "print(data1501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combining all the pre-processing rules in entire data-set of 'Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [00:03<00:00, 1644.22it/s]\n"
     ]
    }
   ],
   "source": [
    "processedReviews = []\n",
    "\n",
    "for sentence in tqdm(finalData['Text'].values):\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
    "    sentence = decontracting(sentence)\n",
    "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
    "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "    processedReviews.append(sentence.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now processing with the same rules summary of every review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [00:03<00:00, 1457.15it/s]\n"
     ]
    }
   ],
   "source": [
    "processedSummary = []\n",
    "\n",
    "for sentence in tqdm(finalData['Summary'].values):\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
    "    sentence = decontracting(sentence)\n",
    "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
    "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "    processedSummary.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aahhhs', 'aback', 'abandon', 'abates', 'abbott', 'abby', 'abdominal', 'abiding', 'ability']\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (4986, 12997)\n",
      "the number of unique words  12997\n"
     ]
    }
   ],
   "source": [
    "countVect = CountVectorizer()\n",
    "countVect.fit_transform(processedReviews)\n",
    "print(countVect.get_feature_names()[:10])\n",
    "\n",
    "finalCount = countVect.transform(processedReviews)\n",
    "print(\"the type of count vectorizer \",type(finalCount))\n",
    "print(\"the shape of out text BOW vectorizer \",finalCount.get_shape())\n",
    "print(\"the number of unique words \", finalCount.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aa', 1), ('aahhhs', 1), ('aback', 1), ('abandon', 1), ('abates', 1), ('abbott', 1), ('abby', 1), ('abdominal', 1), ('abiding', 1), ('ability', 1), ('able', 1), ('abor', 1), ('aboulutely', 1), ('absence', 1), ('absent', 1), ('absoloutely', 1), ('absolute', 1), ('absolutely', 1), ('absolutley', 1), ('absolutly', 1)]\n"
     ]
    }
   ],
   "source": [
    "freqDist = nltk.FreqDist(countVect.get_feature_names())\n",
    "print(freqDist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 4590), ('like', 1985), ('good', 1735), ('great', 1511), ('taste', 1360), ('one', 1314), ('product', 1288), ('would', 1237), ('flavor', 1164), ('love', 1093), ('coffee', 1049), ('food', 1013), ('chips', 1003), ('tea', 885), ('no', 868), ('really', 829), ('get', 805), ('best', 785), ('much', 760), ('amazon', 713)]\n"
     ]
    }
   ],
   "source": [
    "allWords = []\n",
    "for eachSentence in processedReviews:\n",
    "    for word in eachSentence.split(' '):\n",
    "        allWords.append(word)\n",
    "freqDist = nltk.FreqDist(allWords)\n",
    "print(freqDist.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bi-gram, tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (4986, 3142)\n",
      "the number of unique words including both unigrams and bigrams  3142\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\n",
    "final_bigram_counts = count_vect.fit_transform(processedReviews)\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'able find', 'able get', 'absolute', 'absolutely', 'absolutely delicious', 'absolutely love', 'absolutely no', 'according']\n"
     ]
    }
   ],
   "source": [
    "tfIdfVect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tfIdfVect.fit(processedReviews)\n",
    "print(tfIdfVect.get_feature_names()[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(4986, 3142)\n",
      "3142\n"
     ]
    }
   ],
   "source": [
    "finalTfIdfVect = tfIdfVect.transform(processedReviews)\n",
    "print(type(finalTfIdfVect))\n",
    "print(finalTfIdfVect.get_shape())\n",
    "print(finalTfIdfVect.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product', 'available', 'victor', 'traps', 'unreal', 'course', 'total', 'fly', 'genocide', 'pretty', 'stinky', 'right', 'nearby']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "listOfSent = []\n",
    "\n",
    "for item in processedReviews:\n",
    "    temp = item.split(' ')\n",
    "    listOfSent.append(temp)\n",
    "\n",
    "print(listOfSent[0])\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(listOfSent, min_count=5, vector_size=50, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3818\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taste', 0.993860125541687),\n",
       " ('strong', 0.9918496012687683),\n",
       " ('tastes', 0.9872565865516663),\n",
       " ('sweet', 0.9866679310798645),\n",
       " ('bitter', 0.9830662608146667),\n",
       " ('nice', 0.9772301316261292),\n",
       " ('flavor', 0.9746538996696472),\n",
       " ('really', 0.9728959202766418),\n",
       " ('rich', 0.9719640612602234),\n",
       " ('smooth', 0.9694072008132935)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
